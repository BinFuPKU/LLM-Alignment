# LLM-Alignment
这两份报告对大语言模型对齐和对齐算法进行调研.


## 大语言模型对齐调研
### 基本概念和流程
### 数据集构建
#### 公开人类反馈数据集
#### 自动构建高质量数据集
### 安全对齐 (safety alignment)
#### 安全对抗（矛与盾）
#### 越狱攻击（Jailbreaking attack）
#### 对抗越狱攻击（Red Teaming）
### 对齐方法
#### 指令对齐（instruction/prompt）
#### 基于人类反馈的强化学习(RLHF)



## 对齐算法调研（主要为RLHF算法）
### 离线强化学习对齐算法
#### DPO损失函数之各种变种：
  铰链损失（SLIC）、二分类损失（KTO和BCO）、最小二乘损失（IPO）、放松KL约束（CPO）、反向KL散度（EXO）、正负优势比（ORPO）；
  增强正例（ DPOP ）；
  细粒度偏好-列表排序学习（LiPO-𝜆）；
  正负偏好奖励差（ODPO和P3O）；
  负偏好优化（NPO）。
#### 学习策略：
  课程学习（Curry-DPO）；
  稳定训练-限制参数更新幅度（sDPO、TR-DPO）；
#### 数据鲁棒性：
  数据增强（DOVE和RPO）；
  奖励模型和语言模型同时训练（ RLP ）。
#### 多目标（多奖励，如安全）微调对齐：CDPO、 C-DPO和ALARM。
### 在线强化学习对齐算法
#### 对当前模型自身（𝜋_𝜃^𝑡）生成的回复进行打分排序（或选择高质量部分）：
  奖励函数打分排序： RSO、RS-DPO、ReMax、RAFT、D2O、 Cringe Loss
  超大模型打分排序：OAIF
#### 细粒度-列表排序学习：PRO、RRHF
#### 奖励函数： 度量函数（ LD-Align ）
#### 当前模型自我生成回复-反省-改写：RLRF
#### 离线数据和在线数据集结合：DNO、IPO-MD
#### 迭代进化（𝜋_𝜃^𝑡比𝜋_𝜃^(𝑡−1)生成回复要好）：SPIN
#### 红队发现漏洞：
  增强上下文生成安全回复：ITERALIGN
#### 多目标奖励函数：SteerLM
#### 强化学习框架：SENSEI
#### 离线-在线 两阶段：MPO（DPO+PPO）
### 未来研究方向
#### 设计更合理的损失函数：更好的学习目标、更精准的奖励。
#### 更稳定更高效的训练：如离线-在线混合训练。
#### 多目标奖励及平衡：如帕累托最优、能力冲突分析等。
#### 迭代进化学习：如红蓝对抗、课程学习等。
#### 鲁棒性训练：如缓解训练数据噪音、数据不完美问题等。
#### 更全面、高效、可靠的评测体系：可用来检验模型动态训练效果。
